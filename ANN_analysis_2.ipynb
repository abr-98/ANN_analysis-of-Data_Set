{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 2s 501us/step - loss: 0.4474 - acc: 0.7936\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.3983 - acc: 0.8101\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 1s 334us/step - loss: 0.3836 - acc: 0.8142\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 1s 345us/step - loss: 0.3835 - acc: 0.8136\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.3772 - acc: 0.8182\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 2s 377us/step - loss: 0.3760 - acc: 0.8158\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 2s 433us/step - loss: 0.3801 - acc: 0.8139\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 2s 430us/step - loss: 0.3757 - acc: 0.8198\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 2s 457us/step - loss: 0.3693 - acc: 0.8225\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 1s 314us/step - loss: 0.3689 - acc: 0.8249\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.3694 - acc: 0.8225\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 1s 279us/step - loss: 0.3713 - acc: 0.8225\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.3636 - acc: 0.8296\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 1s 275us/step - loss: 0.3669 - acc: 0.8239\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3621 - acc: 0.8289\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 1s 278us/step - loss: 0.3618 - acc: 0.8295\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3578 - acc: 0.8308\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.3583 - acc: 0.8330\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 1s 268us/step - loss: 0.3554 - acc: 0.8361\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 1s 277us/step - loss: 0.3543 - acc: 0.8306\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 1s 277us/step - loss: 0.3565 - acc: 0.8293\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.3590 - acc: 0.8333\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.3522 - acc: 0.8337\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 1s 278us/step - loss: 0.3547 - acc: 0.8343\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 1s 274us/step - loss: 0.3542 - acc: 0.8310\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 1s 277us/step - loss: 0.3528 - acc: 0.8333\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 1s 271us/step - loss: 0.3539 - acc: 0.8345\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.3550 - acc: 0.8314\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 1s 291us/step - loss: 0.3541 - acc: 0.8326\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3538 - acc: 0.8312\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 1s 274us/step - loss: 0.3507 - acc: 0.8320\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 1s 278us/step - loss: 0.3517 - acc: 0.8363\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3467 - acc: 0.8366\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 1s 278us/step - loss: 0.3495 - acc: 0.8347\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 1s 279us/step - loss: 0.3515 - acc: 0.8309\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.3514 - acc: 0.8316\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 1s 276us/step - loss: 0.3487 - acc: 0.8349\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.3485 - acc: 0.8356\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3480 - acc: 0.8373\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 1s 235us/step - loss: 0.3483 - acc: 0.8332\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.3474 - acc: 0.8343\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.3461 - acc: 0.8359\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.3454 - acc: 0.8382\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.3435 - acc: 0.8361\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 1s 279us/step - loss: 0.3444 - acc: 0.8371\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3443 - acc: 0.8365\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 1s 276us/step - loss: 0.3441 - acc: 0.8378\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.3431 - acc: 0.8381\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3432 - acc: 0.8370\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 1s 277us/step - loss: 0.3459 - acc: 0.8363\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 1s 277us/step - loss: 0.3432 - acc: 0.8385\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.3414 - acc: 0.8393\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.3422 - acc: 0.8394\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 1s 276us/step - loss: 0.3423 - acc: 0.8374\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3410 - acc: 0.8373\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.3421 - acc: 0.8400\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 1s 275us/step - loss: 0.3438 - acc: 0.8371\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 1s 294us/step - loss: 0.3416 - acc: 0.8388\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.3442 - acc: 0.8383\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 1s 277us/step - loss: 0.3413 - acc: 0.8411\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 1s 279us/step - loss: 0.3428 - acc: 0.8400\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3414 - acc: 0.8373\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 1s 291us/step - loss: 0.3392 - acc: 0.8393\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.3390 - acc: 0.8384\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 1s 275us/step - loss: 0.3377 - acc: 0.8409\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.3391 - acc: 0.8398\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.3383 - acc: 0.8397\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 1s 301us/step - loss: 0.3356 - acc: 0.8441\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3367 - acc: 0.8410\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.3362 - acc: 0.8430\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3374 - acc: 0.8398\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 1s 278us/step - loss: 0.3373 - acc: 0.8425\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.3340 - acc: 0.8441\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.3385 - acc: 0.8408\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.3362 - acc: 0.8420\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 1s 278us/step - loss: 0.3326 - acc: 0.8445\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 1s 278us/step - loss: 0.3349 - acc: 0.8425\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 1s 294us/step - loss: 0.3369 - acc: 0.8407\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.3329 - acc: 0.8439\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3388 - acc: 0.8405\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.3343 - acc: 0.8456\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3339 - acc: 0.8433\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3351 - acc: 0.8438\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 1s 270us/step - loss: 0.3329 - acc: 0.8433\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 1s 290us/step - loss: 0.3353 - acc: 0.8449\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 1s 274us/step - loss: 0.3312 - acc: 0.8460\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 1s 274us/step - loss: 0.3322 - acc: 0.8455\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3359 - acc: 0.8403\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 1s 288us/step - loss: 0.3326 - acc: 0.8440\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 1s 288us/step - loss: 0.3306 - acc: 0.8435\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 1s 275us/step - loss: 0.3300 - acc: 0.8473\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 1s 276us/step - loss: 0.3312 - acc: 0.8428\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3341 - acc: 0.8437\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 1s 273us/step - loss: 0.3268 - acc: 0.8483\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.3303 - acc: 0.8469\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 1s 288us/step - loss: 0.3273 - acc: 0.8486\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.3285 - acc: 0.8478\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 1s 295us/step - loss: 0.3275 - acc: 0.8476\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 1s 274us/step - loss: 0.3278 - acc: 0.8496\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.3286 - acc: 0.8463\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 1s 275us/step - loss: 0.3274 - acc: 0.8474\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 1s 273us/step - loss: 0.3235 - acc: 0.8490\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.3261 - acc: 0.8488\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 1s 278us/step - loss: 0.3260 - acc: 0.8506\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 1s 279us/step - loss: 0.3232 - acc: 0.8512\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 1s 275us/step - loss: 0.3240 - acc: 0.8478\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 1s 275us/step - loss: 0.3250 - acc: 0.8512\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.3250 - acc: 0.8471\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 1s 300us/step - loss: 0.3286 - acc: 0.8477\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.3291 - acc: 0.8455\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3256 - acc: 0.8476\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3236 - acc: 0.8476\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.3189 - acc: 0.8532\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.3229 - acc: 0.8504\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.3240 - acc: 0.8481\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3222 - acc: 0.8504\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 1s 290us/step - loss: 0.3231 - acc: 0.8500\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 1s 275us/step - loss: 0.3230 - acc: 0.8503\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3229 - acc: 0.8482\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3211 - acc: 0.8492\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.3229 - acc: 0.8475\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 1s 291us/step - loss: 0.3255 - acc: 0.8468\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.3219 - acc: 0.8493\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.3194 - acc: 0.8507\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.3185 - acc: 0.8531\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3171 - acc: 0.8541\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.3188 - acc: 0.8529\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3182 - acc: 0.8510\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3180 - acc: 0.8514\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 1s 275us/step - loss: 0.3189 - acc: 0.8505\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.3167 - acc: 0.8504\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 1s 277us/step - loss: 0.3151 - acc: 0.8520\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.3183 - acc: 0.8488\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 1s 337us/step - loss: 0.3179 - acc: 0.8516\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 1s 293us/step - loss: 0.3150 - acc: 0.8521\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.3166 - acc: 0.8508\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3147 - acc: 0.8547\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3131 - acc: 0.8546\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 1s 293us/step - loss: 0.3118 - acc: 0.8563\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.3194 - acc: 0.8485\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 1s 290us/step - loss: 0.3163 - acc: 0.8512\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3116 - acc: 0.8526\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.3099 - acc: 0.8560\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 1s 294us/step - loss: 0.3104 - acc: 0.8543\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 1s 296us/step - loss: 0.3173 - acc: 0.8526\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.3130 - acc: 0.8549\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 1s 277us/step - loss: 0.3073 - acc: 0.8557\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 1s 279us/step - loss: 0.3094 - acc: 0.8566\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3125 - acc: 0.8557\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.3110 - acc: 0.8575\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 1s 288us/step - loss: 0.3096 - acc: 0.8577\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.3121 - acc: 0.8546\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3099 - acc: 0.8556\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 1s 278us/step - loss: 0.3040 - acc: 0.8590\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 1s 290us/step - loss: 0.3096 - acc: 0.8538\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.3104 - acc: 0.8537\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.3079 - acc: 0.8573\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.3057 - acc: 0.8594\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.3087 - acc: 0.8571\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 1s 301us/step - loss: 0.3058 - acc: 0.8573\n",
      "1066/1066 [==============================] - 0s 255us/step\n",
      "4263/4263 [==============================] - 0s 105us/step\n",
      "\n",
      "acc: 81.97%\n",
      "\n",
      "acc: 84.89%\n",
      "[[127  64   0  31]\n",
      " [ 82 446  14   9]\n",
      " [  6  54  29   0]\n",
      " [ 13   3   0  51]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fast       0.56      0.57      0.56       222\n",
      "      Normal       0.79      0.81      0.80       551\n",
      "        Slow       0.67      0.33      0.44        89\n",
      "   Very Fast       0.56      0.76      0.65        67\n",
      "\n",
      "    accuracy                           0.70       929\n",
      "   macro avg       0.64      0.62      0.61       929\n",
      "weighted avg       0.70      0.70      0.70       929\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7029063509149623"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df[['Class','Mean_speed_kmph']].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test_k = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "new_y_2=y_train[0].copy()\n",
    "new_y_d_2=pd.DataFrame(new_y_2)\n",
    "new_y=y_test_k[0].copy()\n",
    "\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "y_test=pd.DataFrame(new_y)\n",
    "\n",
    "y_train_2=pd.get_dummies(new_y_d_2)\n",
    "y_test_2=pd.get_dummies(new_y)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(600, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "predictions=model.predict(X_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "speed_check=y_test_k[1].copy()\n",
    "#speed_check_d=pd.DataFrame(speed_check)\n",
    "speed_check_l=speed_check.tolist()     #\n",
    "\n",
    "prediction_l=[]\n",
    "count=0\n",
    "all_zero=[]\n",
    "#print(len(predictions))\n",
    "for x in predictions:\n",
    "    k_1=round(x[0])\n",
    "    k_1_i=int(k_1)\n",
    "    k_1_s=str(k_1_i)\n",
    "    k_2=round(x[1])\n",
    "    k_2_i=int(k_2)\n",
    "    k_2_s=str(k_2_i)\n",
    "    k_3=round(x[2])\n",
    "    k_3_i=int(k_3)\n",
    "    k_3_s=str(k_3_i)\n",
    "    k_4=round(x[3])\n",
    "    k_4_i=int(k_4)\n",
    "    k_4_s=str(k_4_i)\n",
    "    if k_1_i==0 and k_2_i==0 and k_3_i==0 and k_4_i==0:\n",
    "        all_zero.append(count)\n",
    "    #print(k_1_s+' '+k_2_s+' '+k_3_s+' '+k_4_s)\n",
    "    if k_1_i==1:\n",
    "        prediction_l.append('Fast')\n",
    "    if k_2_i==1:\n",
    "        prediction_l.append('Normal')\n",
    "    if k_3_i==1:\n",
    "        prediction_l.append('Slow')\n",
    "    if k_4_i==1:\n",
    "        prediction_l.append('Very Fast')\n",
    "    count=count+1\n",
    "    \n",
    "#print(all_zero)\n",
    "#print(len(all_zero))\n",
    "#print(prediction_l)\n",
    "#print(len(prediction_l))\n",
    "\n",
    "y_test_l=[]\n",
    "\n",
    "l=len(y_test_2)\n",
    "j=0\n",
    "while j<l:\n",
    "    if j not in all_zero:\n",
    "        if y_test_2.iloc[j][0]==1:\n",
    "            y_test_l.append('Fast')\n",
    "        if y_test_2.iloc[j][1]==1:\n",
    "            y_test_l.append('Normal')\n",
    "        if y_test_2.iloc[j][2]==1:\n",
    "            y_test_l.append('Slow')\n",
    "        if y_test_2.iloc[j][3]==1:\n",
    "            y_test_l.append('Very Fast')\n",
    "    j=j+1\n",
    "    #print(i)\n",
    "l_2=len(y_test_l)\n",
    "j=0\n",
    "while j<l_2:\n",
    "    if speed_check_l[j]>=17 and speed_check_l[j]<=23 :\n",
    "        if y_test_l[j]=='Normal' and prediction_l[j]=='Slow':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "        if y_test_l[j]=='Slow' and prediction_l[j]=='Normal':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "    if speed_check_l[j]>=32 and speed_check_l[j]<=38 :\n",
    "        if y_test_l[j]=='Normal' and prediction_l[j]=='Fast':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "        if y_test_l[j]=='Fast' and prediction_l[j]=='Normal':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "    if speed_check_l[j]>=47 and speed_check_l[j]<=53 :\n",
    "        if y_test_l[j]=='Very Fast' and prediction_l[j]=='Fast':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "        if y_test_l[j]=='Fast' and prediction_l[j]=='Very Fast':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "    j=j+1\n",
    "#print(y_test_l)\n",
    "j=0\n",
    "right=0\n",
    "while j<l_2:\n",
    "    if y_test_l[j]==prediction_l[j]:\n",
    "        right=right+1\n",
    "    j=j+1;\n",
    "    #print(j)\n",
    "#print(right)\n",
    "#print(right/len(y_test))\n",
    "a=confusion_matrix(y_test_l,prediction_l)\n",
    "print(a)\n",
    "\n",
    "print(classification_report(y_test_l,prediction_l))\n",
    "accuracy_score(y_test_l,prediction_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 2s 544us/step - loss: 0.4446 - acc: 0.7986\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 1s 288us/step - loss: 0.3993 - acc: 0.8088\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.3932 - acc: 0.8114\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 1s 297us/step - loss: 0.3868 - acc: 0.8105\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.3789 - acc: 0.8177\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 1s 296us/step - loss: 0.3789 - acc: 0.8214\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 1s 314us/step - loss: 0.3743 - acc: 0.8212\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.3738 - acc: 0.8204\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 1s 336us/step - loss: 0.3731 - acc: 0.8224\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 1s 304us/step - loss: 0.3694 - acc: 0.8240\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.3693 - acc: 0.8225\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.3672 - acc: 0.8267\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 1s 308us/step - loss: 0.3678 - acc: 0.8247\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.3660 - acc: 0.8245\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 1s 303us/step - loss: 0.3637 - acc: 0.8279\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.3617 - acc: 0.8305\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.3604 - acc: 0.8300\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.3612 - acc: 0.8269\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3595 - acc: 0.8327\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.3554 - acc: 0.8310\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 1s 312us/step - loss: 0.3571 - acc: 0.8303\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.3556 - acc: 0.8313\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.3579 - acc: 0.8337\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 1s 334us/step - loss: 0.3565 - acc: 0.8281\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 1s 296us/step - loss: 0.3556 - acc: 0.8319\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 1s 293us/step - loss: 0.3557 - acc: 0.8307\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 2s 370us/step - loss: 0.3561 - acc: 0.8332\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.3520 - acc: 0.8335\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 1s 336us/step - loss: 0.3503 - acc: 0.8354\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 1s 347us/step - loss: 0.3518 - acc: 0.8302\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.3534 - acc: 0.8318\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 1s 335us/step - loss: 0.3516 - acc: 0.8348\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 1s 341us/step - loss: 0.3490 - acc: 0.8344\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 2s 482us/step - loss: 0.3483 - acc: 0.8325\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 2s 411us/step - loss: 0.3486 - acc: 0.8349\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 1s 300us/step - loss: 0.3498 - acc: 0.8359\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.3466 - acc: 0.8374\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 1s 306us/step - loss: 0.3485 - acc: 0.8358\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.3502 - acc: 0.8324\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3470 - acc: 0.8358\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.3477 - acc: 0.8380\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 2s 451us/step - loss: 0.3499 - acc: 0.8337\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 2s 398us/step - loss: 0.3467 - acc: 0.8352\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 2s 359us/step - loss: 0.3459 - acc: 0.8357\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.3474 - acc: 0.8356\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 2s 377us/step - loss: 0.3444 - acc: 0.8377\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 1s 329us/step - loss: 0.3432 - acc: 0.8366\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 1s 345us/step - loss: 0.3469 - acc: 0.8348\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.3418 - acc: 0.8386\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 1s 303us/step - loss: 0.3456 - acc: 0.8385\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.3427 - acc: 0.8415\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3434 - acc: 0.8373\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 1s 293us/step - loss: 0.3444 - acc: 0.8370\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.3425 - acc: 0.8394\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.3430 - acc: 0.8376\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 2s 446us/step - loss: 0.3417 - acc: 0.8386\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 2s 453us/step - loss: 0.3449 - acc: 0.8355\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 2s 484us/step - loss: 0.3427 - acc: 0.8394\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 2s 352us/step - loss: 0.3427 - acc: 0.8371\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 1s 351us/step - loss: 0.3417 - acc: 0.8385\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.3419 - acc: 0.8396\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 2s 368us/step - loss: 0.3396 - acc: 0.8388\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 2s 466us/step - loss: 0.3426 - acc: 0.8391\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.3411 - acc: 0.8400\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.3431 - acc: 0.8362\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 2s 361us/step - loss: 0.3394 - acc: 0.8384\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 2s 507us/step - loss: 0.3411 - acc: 0.8397\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 2s 401us/step - loss: 0.3397 - acc: 0.8383\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.3377 - acc: 0.8395\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 2s 380us/step - loss: 0.3402 - acc: 0.8405\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.3396 - acc: 0.8380\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 1s 303us/step - loss: 0.3390 - acc: 0.8418\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 1s 298us/step - loss: 0.3379 - acc: 0.8417\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.3399 - acc: 0.8405\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.3410 - acc: 0.8413\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.3376 - acc: 0.8414\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 1s 290us/step - loss: 0.3384 - acc: 0.8425\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 1s 295us/step - loss: 0.3388 - acc: 0.8388\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 1s 299us/step - loss: 0.3347 - acc: 0.8446\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 1s 291us/step - loss: 0.3365 - acc: 0.8421\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 1s 295us/step - loss: 0.3431 - acc: 0.8356\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 1s 304us/step - loss: 0.3388 - acc: 0.8406\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 1s 311us/step - loss: 0.3367 - acc: 0.8420\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.3344 - acc: 0.8426\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 1s 291us/step - loss: 0.3390 - acc: 0.8408\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.3347 - acc: 0.8430\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.3370 - acc: 0.8415\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 1s 278us/step - loss: 0.3350 - acc: 0.8446\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 1s 316us/step - loss: 0.3339 - acc: 0.8440\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.3348 - acc: 0.8435\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.3328 - acc: 0.8427\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.3353 - acc: 0.8427\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 1s 295us/step - loss: 0.3320 - acc: 0.8444\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.3339 - acc: 0.8442\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 2s 418us/step - loss: 0.3312 - acc: 0.8471\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.3343 - acc: 0.8417\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.3338 - acc: 0.8456\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 1s 299us/step - loss: 0.3309 - acc: 0.8453\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 1s 308us/step - loss: 0.3296 - acc: 0.8441\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 1s 306us/step - loss: 0.3319 - acc: 0.8442\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.3319 - acc: 0.8425\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.3321 - acc: 0.8451\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 1s 302us/step - loss: 0.3285 - acc: 0.8458\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 1s 341us/step - loss: 0.3334 - acc: 0.8438\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 2s 380us/step - loss: 0.3344 - acc: 0.8415\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 2s 449us/step - loss: 0.3312 - acc: 0.8459\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 2s 495us/step - loss: 0.3284 - acc: 0.8439\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 1s 298us/step - loss: 0.3248 - acc: 0.8484\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 1s 301us/step - loss: 0.3293 - acc: 0.8473\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 1s 293us/step - loss: 0.3280 - acc: 0.8441\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.3269 - acc: 0.8465\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 1s 309us/step - loss: 0.3277 - acc: 0.8456\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 1s 298us/step - loss: 0.3266 - acc: 0.8493\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 1s 308us/step - loss: 0.3281 - acc: 0.8459\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 1s 300us/step - loss: 0.3267 - acc: 0.8462\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 1s 303us/step - loss: 0.3271 - acc: 0.8471\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 1s 297us/step - loss: 0.3306 - acc: 0.8407\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 1s 295us/step - loss: 0.3245 - acc: 0.8458\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 1s 298us/step - loss: 0.3247 - acc: 0.8482\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 1s 310us/step - loss: 0.3255 - acc: 0.8486\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 1s 295us/step - loss: 0.3275 - acc: 0.8465\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 1s 304us/step - loss: 0.3257 - acc: 0.8443\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 1s 308us/step - loss: 0.3244 - acc: 0.8496\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 1s 311us/step - loss: 0.3234 - acc: 0.8497\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 1s 313us/step - loss: 0.3221 - acc: 0.8492\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.3203 - acc: 0.8506\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 1s 299us/step - loss: 0.3208 - acc: 0.8506\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 1s 300us/step - loss: 0.3231 - acc: 0.8483\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 1s 302us/step - loss: 0.3218 - acc: 0.8491\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 1s 299us/step - loss: 0.3247 - acc: 0.8486\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 1s 297us/step - loss: 0.3289 - acc: 0.8450\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 1s 297us/step - loss: 0.3218 - acc: 0.8490\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 1s 310us/step - loss: 0.3218 - acc: 0.8520\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.3226 - acc: 0.8480\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 1s 303us/step - loss: 0.3258 - acc: 0.8482\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 1s 302us/step - loss: 0.3218 - acc: 0.8503\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.3183 - acc: 0.8510\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 1s 305us/step - loss: 0.3158 - acc: 0.8534\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 1s 301us/step - loss: 0.3240 - acc: 0.8484\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 1s 290us/step - loss: 0.3196 - acc: 0.8498\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 1s 301us/step - loss: 0.3183 - acc: 0.8509\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 1s 297us/step - loss: 0.3213 - acc: 0.8485\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 1s 300us/step - loss: 0.3182 - acc: 0.8520\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 1s 299us/step - loss: 0.3188 - acc: 0.8510\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 1s 295us/step - loss: 0.3159 - acc: 0.8531\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.3104 - acc: 0.8542\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 1s 294us/step - loss: 0.3210 - acc: 0.8503\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 1s 303us/step - loss: 0.3208 - acc: 0.8509\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 1s 296us/step - loss: 0.3170 - acc: 0.8524\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.3171 - acc: 0.8512\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 1s 298us/step - loss: 0.3186 - acc: 0.8510\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 1s 291us/step - loss: 0.3122 - acc: 0.8519\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 1s 306us/step - loss: 0.3173 - acc: 0.8513\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 1s 301us/step - loss: 0.3160 - acc: 0.8545\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 1s 302us/step - loss: 0.3157 - acc: 0.8525\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 1s 305us/step - loss: 0.3183 - acc: 0.8527\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 1s 304us/step - loss: 0.3167 - acc: 0.8517\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 1s 298us/step - loss: 0.3127 - acc: 0.8563\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 1s 297us/step - loss: 0.3108 - acc: 0.8557\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 1s 293us/step - loss: 0.3180 - acc: 0.8482\n",
      "1066/1066 [==============================] - 0s 354us/step\n",
      "4263/4263 [==============================] - 0s 92us/step\n",
      "\n",
      "acc: 83.56%\n",
      "[[108 104   1  20]\n",
      " [ 40 520  13   3]\n",
      " [  1  50  38   0]\n",
      " [ 13   5   0  43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fast       0.67      0.46      0.55       233\n",
      "      Normal       0.77      0.90      0.83       576\n",
      "        Slow       0.73      0.43      0.54        89\n",
      "   Very Fast       0.65      0.70      0.68        61\n",
      "\n",
      "    accuracy                           0.74       959\n",
      "   macro avg       0.70      0.62      0.65       959\n",
      "weighted avg       0.73      0.74      0.72       959\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7393117831074035"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df[['Class','Mean_speed_kmph']].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test_k = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "new_y_2=y_train[0].copy()\n",
    "new_y_d_2=pd.DataFrame(new_y_2)\n",
    "new_y=y_test_k[0].copy()\n",
    "\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "y_test=pd.DataFrame(new_y)\n",
    "\n",
    "y_train_2=pd.get_dummies(new_y_d_2)\n",
    "y_test_2=pd.get_dummies(new_y)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(600, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "predictions=model.predict(X_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "speed_check=y_test_k[1].copy()\n",
    "#speed_check_d=pd.DataFrame(speed_check)\n",
    "speed_check_l=speed_check.tolist()     #\n",
    "\n",
    "prediction_l=[]\n",
    "count=0\n",
    "all_zero=[]\n",
    "#print(len(predictions))\n",
    "for x in predictions:\n",
    "    k_1=round(x[0])\n",
    "    k_1_i=int(k_1)\n",
    "    k_1_s=str(k_1_i)\n",
    "    k_2=round(x[1])\n",
    "    k_2_i=int(k_2)\n",
    "    k_2_s=str(k_2_i)\n",
    "    k_3=round(x[2])\n",
    "    k_3_i=int(k_3)\n",
    "    k_3_s=str(k_3_i)\n",
    "    k_4=round(x[3])\n",
    "    k_4_i=int(k_4)\n",
    "    k_4_s=str(k_4_i)\n",
    "    if k_1_i==0 and k_2_i==0 and k_3_i==0 and k_4_i==0:\n",
    "        all_zero.append(count)\n",
    "    #print(k_1_s+' '+k_2_s+' '+k_3_s+' '+k_4_s)\n",
    "    if k_1_i==1:\n",
    "        prediction_l.append('Fast')\n",
    "    if k_2_i==1:\n",
    "        prediction_l.append('Normal')\n",
    "    if k_3_i==1:\n",
    "        prediction_l.append('Slow')\n",
    "    if k_4_i==1:\n",
    "        prediction_l.append('Very Fast')\n",
    "    count=count+1\n",
    "    \n",
    "#print(all_zero)\n",
    "#print(len(all_zero))\n",
    "#print(prediction_l)\n",
    "#print(len(prediction_l))\n",
    "\n",
    "y_test_l=[]\n",
    "\n",
    "l=len(y_test_2)\n",
    "j=0\n",
    "while j<l:\n",
    "    if j not in all_zero:\n",
    "        if y_test_2.iloc[j][0]==1:\n",
    "            y_test_l.append('Fast')\n",
    "        if y_test_2.iloc[j][1]==1:\n",
    "            y_test_l.append('Normal')\n",
    "        if y_test_2.iloc[j][2]==1:\n",
    "            y_test_l.append('Slow')\n",
    "        if y_test_2.iloc[j][3]==1:\n",
    "            y_test_l.append('Very Fast')\n",
    "    j=j+1\n",
    "    #print(i)\n",
    "l_2=len(y_test_l)\n",
    "j=0\n",
    "while j<l_2:\n",
    "    if speed_check_l[j]>=17 and speed_check_l[j]<=23 :\n",
    "        if y_test_l[j]=='Normal' and prediction_l[j]=='Slow':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "        if y_test_l[j]=='Slow' and prediction_l[j]=='Normal':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "    if speed_check_l[j]>=32 and speed_check_l[j]<=38 :\n",
    "        if y_test_l[j]=='Normal' and prediction_l[j]=='Fast':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "        if y_test_l[j]=='Fast' and prediction_l[j]=='Normal':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "    if speed_check_l[j]>=47 and speed_check_l[j]<=53 :\n",
    "        if y_test_l[j]=='Very Fast' and prediction_l[j]=='Fast':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "        if y_test_l[j]=='Fast' and prediction_l[j]=='Very Fast':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "    j=j+1\n",
    "#print(y_test_l)\n",
    "j=0\n",
    "right=0\n",
    "while j<l_2:\n",
    "    if y_test_l[j]==prediction_l[j]:\n",
    "        right=right+1\n",
    "    j=j+1;\n",
    "    #print(j)\n",
    "#print(right)\n",
    "#print(right/len(y_test))\n",
    "a=confusion_matrix(y_test_l,prediction_l)\n",
    "print(a)\n",
    "\n",
    "print(classification_report(y_test_l,prediction_l))\n",
    "accuracy_score(y_test_l,prediction_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0        1\n",
      "1323     Normal  32.8906\n",
      "1839     Normal  27.1147\n",
      "798      Normal  25.3206\n",
      "3855     Normal  22.1571\n",
      "4552     Normal  26.8351\n",
      "856      Normal   34.532\n",
      "2333  Very Fast  67.2025\n",
      "2499     Normal  30.2745\n",
      "5010     Normal  29.9627\n",
      "4379     Normal  27.2221\n",
      "4733     Normal  20.7327\n",
      "655      Normal  30.5468\n",
      "2101  Very Fast  65.8472\n",
      "893      Normal  30.4039\n",
      "373      Normal  31.9208\n",
      "297      Normal  29.7434\n",
      "3817       Slow  17.4819\n",
      "1808     Normal  28.2495\n",
      "4392     Normal  20.8796\n",
      "3585       Slow  11.3372\n",
      "240      Normal  29.7452\n",
      "4650     Normal   23.261\n",
      "2951     Normal  33.9055\n",
      "2860       Fast  48.8663\n",
      "465      Normal  34.0504\n",
      "3685       Fast  40.2017\n",
      "4316     Normal  20.5973\n",
      "84       Normal   26.149\n",
      "1498     Normal  33.2387\n",
      "803      Normal  28.6815\n",
      "...         ...      ...\n",
      "1700       Fast  40.7511\n",
      "745      Normal  30.0088\n",
      "5074  Very Fast  56.1609\n",
      "1815     Normal  33.6603\n",
      "1611     Normal  27.8781\n",
      "195      Normal  32.8014\n",
      "4898     Normal  28.6067\n",
      "3039     Normal  26.1896\n",
      "802      Normal   32.394\n",
      "3437       Slow  19.1137\n",
      "1717       Fast  45.5117\n",
      "1103     Normal  27.7598\n",
      "3463     Normal  28.8778\n",
      "3358     Normal  23.2036\n",
      "3326       Fast  40.3134\n",
      "1747     Normal  33.0536\n",
      "1740       Fast  38.8713\n",
      "644        Fast  38.3068\n",
      "4145       Fast  42.3307\n",
      "3916       Slow  18.7451\n",
      "120      Normal  29.9765\n",
      "3196     Normal  22.7978\n",
      "1881     Normal   29.029\n",
      "3999     Normal  27.9225\n",
      "4293     Normal  29.5863\n",
      "2841       Fast  35.7972\n",
      "5207       Slow  13.0979\n",
      "1965       Slow  18.8104\n",
      "4537  Very Fast  64.4044\n",
      "134      Normal  31.7407\n",
      "\n",
      "[1066 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Fast  Normal  Slow  Very Fast\n",
      "1323     0       1     0          0\n",
      "1839     0       1     0          0\n",
      "798      0       1     0          0\n",
      "3855     0       1     0          0\n",
      "4552     0       1     0          0\n",
      "856      0       1     0          0\n",
      "2333     0       0     0          1\n",
      "2499     0       1     0          0\n",
      "5010     0       1     0          0\n",
      "4379     0       1     0          0\n",
      "4733     0       1     0          0\n",
      "655      0       1     0          0\n",
      "2101     0       0     0          1\n",
      "893      0       1     0          0\n",
      "373      0       1     0          0\n",
      "297      0       1     0          0\n",
      "3817     0       0     1          0\n",
      "1808     0       1     0          0\n",
      "4392     0       1     0          0\n",
      "3585     0       0     1          0\n",
      "240      0       1     0          0\n",
      "4650     0       1     0          0\n",
      "2951     0       1     0          0\n",
      "2860     1       0     0          0\n",
      "465      0       1     0          0\n",
      "3685     1       0     0          0\n",
      "4316     0       1     0          0\n",
      "84       0       1     0          0\n",
      "1498     0       1     0          0\n",
      "803      0       1     0          0\n",
      "...    ...     ...   ...        ...\n",
      "1700     1       0     0          0\n",
      "745      0       1     0          0\n",
      "5074     0       0     0          1\n",
      "1815     0       1     0          0\n",
      "1611     0       1     0          0\n",
      "195      0       1     0          0\n",
      "4898     0       1     0          0\n",
      "3039     0       1     0          0\n",
      "802      0       1     0          0\n",
      "3437     0       0     1          0\n",
      "1717     1       0     0          0\n",
      "1103     0       1     0          0\n",
      "3463     0       1     0          0\n",
      "3358     0       1     0          0\n",
      "3326     1       0     0          0\n",
      "1747     0       1     0          0\n",
      "1740     1       0     0          0\n",
      "644      1       0     0          0\n",
      "4145     1       0     0          0\n",
      "3916     0       0     1          0\n",
      "120      0       1     0          0\n",
      "3196     0       1     0          0\n",
      "1881     0       1     0          0\n",
      "3999     0       1     0          0\n",
      "4293     0       1     0          0\n",
      "2841     1       0     0          0\n",
      "5207     0       0     1          0\n",
      "1965     0       0     1          0\n",
      "4537     0       0     0          1\n",
      "134      0       1     0          0\n",
      "\n",
      "[1066 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-57082e6af367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'Very Fast'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprediction_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'Fast'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprediction_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0my_test_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'Fast'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprediction_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'Very Fast'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mprediction_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "while j<l:\n",
    "    if j not in all_zero:\n",
    "        if y_test_2.iloc[j]['Fast']==1:\n",
    "            y_test_l.append('Fast')\n",
    "        if y_test_2.iloc[j]['Normal']==1:\n",
    "            y_test_l.append('Normal')\n",
    "        if y_test_2.iloc[j]['Slow']==1:\n",
    "            y_test_l.append('Slow')\n",
    "        if y_test_2.iloc[j]['Very Fast']==1:\n",
    "            y_test_l.append('Very Fast')\n",
    "    j=j+1\n",
    "    #print(i)\n",
    "l_2=len(y_test_l)\n",
    "j=0\n",
    "while j<l_2:\n",
    "    if speed_check_l[j]>=17 and speed_check_l[j]<=23 :\n",
    "        if y_test_l[j]=='Normal' and prediction_l[j]=='Slow':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "        if y_test_l[j]=='Slow' and prediction_l[j]=='Normal':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "    if speed_check_l[j]>=32 and speed_check_l[j]<=38 :\n",
    "        if y_test_l[j]=='Normal' and prediction_l[j]=='Fast':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "        if y_test_l[j]=='Fast' and prediction_l[j]=='Normal':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "    if speed_check_l[j]>=47 and speed_check_l[j]<=53 :\n",
    "        if y_test_l[j]=='Very Fast' and prediction_l[j]=='Fast':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "        if y_test_l[j]=='Fast' and prediction_l[j]=='Very Fast':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "    j=j+1\n",
    "#print(y_test_l)\n",
    "j=0\n",
    "right=0\n",
    "while j<l_2:\n",
    "    if y_test_l[j]==prediction_l[j]:\n",
    "        right=right+1\n",
    "    j=j+1;\n",
    "    #print(j)\n",
    "#print(right)\n",
    "#print(right/len(y_test))\n",
    "a=confusion_matrix(y_test_l,prediction_l)\n",
    "print(a)\n",
    "\n",
    "print(classification_report(y_test_l,prediction_l))\n",
    "accuracy_score(y_test_l,prediction_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
